sonar_array = sonar_set.values
#print(sonar_array)
X = sonar_array[:,0: 60].astype(float)
Y = sonar_array[:, 60]
#Splitting the dataset into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

#appending imported models into a list
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
#Finding the best models to use
results = []
names = []
fold_nums = 10
seed = 7
scoring = 'accuracy'
for name, model in models:
    kfold = KFold(n_splits = fold_nums)
    val_result = cross_val_score(model, X_train, y_train, cv = kfold, scoring = scoring)
    results.append(val_result)
    names.append(name)
    msg = "%s: %f (%f)" % (name, val_result.mean(), val_result.std())
    print("Before Scaling:",msg)
    
#Scaling the dataset before fitting into our model and check for increased performance
pipelines = []
pipelines.append(('Scaled_logReg', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))
pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])))
pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))
pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))
pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))
pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))

output = []
identifiers = []

for name, model in pipelines:
    kfold = KFold(n_splits =  fold_nums, shuffle = True, random_state = 42 )
    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = scoring)
    output.append(cv_results)
    identifiers.append(name)
    msg =  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    #print("After Scaling:",msg)
    
#The result shows that the KNN and the SVM will do best in the model selection

#Tuning the KNN, using grid search to find the the best parameter to use
scaler = StandardScaler().fit(X_train)
rescaled_X_train = scaler.transform(X_train)
neighbors = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]
param_grid = dict(n_neighbors = neighbors)
model = KNeighborsClassifier()
kfold = KFold(n_splits = fold_nums)
grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring, cv = kfold)
grid_result = grid.fit(rescaled_X_train,  y_train)
print("Best: %f using %s " % (grid_result.best_score_, grid_result.best_params_))

means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
print(means)
print(stds)


#Tunning SVC, specifically changing the kernel type and C value, using grid search to find the parameter
scaler = StandardScaler().fit(X_train)
rescaled_X_train = scaler.transform(X_train)
c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]
kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']
param_grid = dict(C = c_values, kernel = kernel_values)
model = SVC()
kfold = KFold(n_splits = fold_nums)
grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring, cv = kfold)
grid_result = grid.fit(rescaled_X_train, y_train)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
